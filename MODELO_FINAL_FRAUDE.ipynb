{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SISTEMA HÍBRIDO DE DETECCIÓN DE FRAUDE CREDITICIO\n",
    "### Integración Optimizada: Isolation Forest + Autoencoder + LSTM + Semi-Supervisado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PASO 1: IMPORTAR LIBRERÍAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librerías cargadas correctamente\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve,\n",
    "    precision_recall_curve, f1_score, precision_score, recall_score,\n",
    "    accuracy_score, average_precision_score\n",
    ")\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, RepeatVector, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "print(\"Librerías cargadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PASO 2: CARGAR DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "INFORMACIÓN DEL DATASET\n",
      "================================================================================\n",
      "Registros: 307,511\n",
      "Variables: 122\n",
      "\n",
      "Distribución TARGET:\n",
      "  Normal (0): 282,686 (91.93%)\n",
      "  Fraude (1): 24,825 (8.07%)\n",
      "\n",
      "Usando muestra estratificada: 50,000 registros\n"
     ]
    }
   ],
   "source": [
    "ruta_datos = \"data/data/application_train.csv\"\n",
    "datos = pd.read_csv(ruta_datos)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"INFORMACIÓN DEL DATASET\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Registros: {len(datos):,}\")\n",
    "print(f\"Variables: {datos.shape[1]}\")\n",
    "print(f\"\\nDistribución TARGET:\")\n",
    "print(f\"  Normal (0): {(datos['TARGET']==0).sum():,} ({(datos['TARGET']==0).mean()*100:.2f}%)\")\n",
    "print(f\"  Fraude (1): {(datos['TARGET']==1).sum():,} ({(datos['TARGET']==1).mean()*100:.2f}%)\")\n",
    "\n",
    "# Usar muestra estratificada para acelerar desarrollo\n",
    "TAMAÑO_MUESTRA = 50000\n",
    "if len(datos) > TAMAÑO_MUESTRA:\n",
    "    datos, _ = train_test_split(datos, train_size=TAMAÑO_MUESTRA, random_state=42, stratify=datos['TARGET'])\n",
    "    print(f\"\\nUsando muestra estratificada: {TAMAÑO_MUESTRA:,} registros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PASO 3: FEATURE ENGINEERING AVANZADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FEATURE ENGINEERING\n",
      "================================================================================\n",
      "Feature Engineering completado\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FEATURE ENGINEERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df = datos.copy()\n",
    "\n",
    "# Variables temporales\n",
    "df['edad'] = -df['DAYS_BIRTH'] / 365\n",
    "df['años_empleado'] = -df['DAYS_EMPLOYED'] / 365\n",
    "df['años_empleado'] = df['años_empleado'].replace(1000.67, np.nan)\n",
    "df['dias_registro'] = -df['DAYS_REGISTRATION']\n",
    "df['dias_id_publicacion'] = -df['DAYS_ID_PUBLISH']\n",
    "\n",
    "# Ratios financieros clave\n",
    "df['ratio_credito_ingreso'] = df['AMT_CREDIT'] / (df['AMT_INCOME_TOTAL'] + 1)\n",
    "df['ratio_anualidad_ingreso'] = df['AMT_ANNUITY'] / (df['AMT_INCOME_TOTAL'] + 1)\n",
    "df['ratio_anualidad_credito'] = df['AMT_ANNUITY'] / (df['AMT_CREDIT'] + 1)\n",
    "df['ratio_bienes_credito'] = df['AMT_GOODS_PRICE'] / (df['AMT_CREDIT'] + 1)\n",
    "\n",
    "# Variables per capita\n",
    "df['ingreso_per_capita'] = df['AMT_INCOME_TOTAL'] / (df['CNT_FAM_MEMBERS'] + 1)\n",
    "df['credito_per_capita'] = df['AMT_CREDIT'] / (df['CNT_FAM_MEMBERS'] + 1)\n",
    "\n",
    "# Indicadores de riesgo\n",
    "df['ratio_edad_empleo'] = df['años_empleado'] / (df['edad'] + 1)\n",
    "df['tiene_telefono'] = df['FLAG_MOBIL'].fillna(0)\n",
    "df['tiene_email'] = df['FLAG_EMAIL'].fillna(0)\n",
    "df['tiene_trabajo_telefono'] = df['FLAG_WORK_PHONE'].fillna(0)\n",
    "\n",
    "# Inconsistencias lógicas (señales de fraude)\n",
    "df['inconsistencia_empleo_edad'] = (df['años_empleado'] > df['edad']).astype(int)\n",
    "df['inconsistencia_ingreso_alto'] = ((df['AMT_INCOME_TOTAL'] > 500000) & (df['REGION_RATING_CLIENT'] == 3)).astype(int)\n",
    "\n",
    "# Codificar categóricas importantes\n",
    "variables_categoricas = [\n",
    "    'NAME_CONTRACT_TYPE', 'CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY',\n",
    "    'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE'\n",
    "]\n",
    "\n",
    "label_encoders = {}\n",
    "for col in variables_categoricas:\n",
    "    if col in df.columns:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = df[col].fillna('Desconocido')\n",
    "        df[f'{col}_cod'] = le.fit_transform(df[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "\n",
    "print(\"Feature Engineering completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PASO 4: SELECCIÓN Y PREPARACIÓN DE VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables finales: 37\n",
      "Nulos restantes: 0\n"
     ]
    }
   ],
   "source": [
    "variables_numericas = [\n",
    "    'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY', 'AMT_GOODS_PRICE',\n",
    "    'ratio_credito_ingreso', 'ratio_anualidad_ingreso', 'ratio_anualidad_credito',\n",
    "    'ratio_bienes_credito', 'ingreso_per_capita', 'credito_per_capita',\n",
    "    'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3',\n",
    "    'edad', 'años_empleado', 'dias_registro', 'dias_id_publicacion',\n",
    "    'CNT_CHILDREN', 'CNT_FAM_MEMBERS',\n",
    "    'REGION_RATING_CLIENT', 'REGION_RATING_CLIENT_W_CITY',\n",
    "    'REG_REGION_NOT_LIVE_REGION', 'REG_CITY_NOT_WORK_CITY',\n",
    "    'ratio_edad_empleo', 'tiene_telefono', 'tiene_email', 'tiene_trabajo_telefono',\n",
    "    'inconsistencia_empleo_edad', 'inconsistencia_ingreso_alto'\n",
    "]\n",
    "\n",
    "variables_categoricas_cod = [f'{col}_cod' for col in variables_categoricas if f'{col}_cod' in df.columns]\n",
    "variables_finales = [v for v in variables_numericas if v in df.columns] + variables_categoricas_cod\n",
    "\n",
    "datos_modelo = df[variables_finales + ['TARGET', 'SK_ID_CURR']].copy()\n",
    "\n",
    "# Imputar nulos\n",
    "for col in variables_finales:\n",
    "    if datos_modelo[col].isnull().sum() > 0:\n",
    "        if col in ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']:\n",
    "            datos_modelo[col].fillna(datos_modelo[col].median(), inplace=True)\n",
    "        else:\n",
    "            datos_modelo[col].fillna(0, inplace=True)\n",
    "\n",
    "print(f\"Variables finales: {len(variables_finales)}\")\n",
    "print(f\"Nulos restantes: {datos_modelo[variables_finales].isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PASO 5: PREPARAR DATOS Y NORMALIZAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparación de datos completada\n",
      "Total muestras: 50,000\n",
      "Train: 35,000 | Test: 15,000\n",
      "Features: 37\n"
     ]
    }
   ],
   "source": [
    "X = datos_modelo[variables_finales].values\n",
    "y = datos_modelo['TARGET'].values\n",
    "ids = datos_modelo['SK_ID_CURR'].values\n",
    "\n",
    "# Normalizar con RobustScaler (mejor con outliers)\n",
    "escalador = RobustScaler()\n",
    "X_escalado = escalador.fit_transform(X)\n",
    "\n",
    "# Separar para entrenamiento de modelos no supervisados\n",
    "X_normales = X_escalado[y == 0]\n",
    "y_normales = y[y == 0]\n",
    "\n",
    "# Split para validación\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_escalado, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Preparación de datos completada\")\n",
    "print(f\"Total muestras: {len(X_escalado):,}\")\n",
    "print(f\"Train: {len(X_train):,} | Test: {len(X_test):,}\")\n",
    "print(f\"Features: {X_escalado.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PASO 6: MODELO 1 - ISOLATION FOREST OPTIMIZADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MODELO 1: ISOLATION FOREST\n",
      "================================================================================\n",
      "\n",
      "Resultados Isolation Forest:\n",
      "  ROC-AUC: 0.4844\n",
      "  F1-Score: 0.0828\n",
      "  Precision: 0.0692\n",
      "  Recall: 0.1032\n",
      "  Umbral óptimo: 0.4626\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MODELO 1: ISOLATION FOREST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Entrenar con parámetros optimizados\n",
    "modelo_if = IsolationForest(\n",
    "    n_estimators=200,\n",
    "    max_samples='auto',\n",
    "    contamination=0.08,  # Aproximado al porcentaje real de fraude\n",
    "    max_features=1.0,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "modelo_if.fit(X_train)\n",
    "\n",
    "# Obtener scores (decision_function: más negativo = más anómalo)\n",
    "scores_if_train = modelo_if.decision_function(X_train)\n",
    "scores_if_test = modelo_if.decision_function(X_test)\n",
    "\n",
    "# Normalizar scores a [0,1] donde 1 = más anómalo\n",
    "def normalizar_scores(scores):\n",
    "    return (scores.max() - scores) / (scores.max() - scores.min())\n",
    "\n",
    "scores_if_train_norm = normalizar_scores(scores_if_train)\n",
    "scores_if_test_norm = normalizar_scores(scores_if_test)\n",
    "\n",
    "# Buscar mejor umbral en train\n",
    "umbrales = np.percentile(scores_if_train_norm, np.arange(85, 100, 1))\n",
    "mejor_f1_if = 0\n",
    "mejor_umbral_if = 0\n",
    "\n",
    "for umbral in umbrales:\n",
    "    pred = (scores_if_train_norm > umbral).astype(int)\n",
    "    f1 = f1_score(y_train, pred, zero_division=0)\n",
    "    if f1 > mejor_f1_if:\n",
    "        mejor_f1_if = f1\n",
    "        mejor_umbral_if = umbral\n",
    "\n",
    "# Evaluar en test\n",
    "pred_if_test = (scores_if_test_norm > mejor_umbral_if).astype(int)\n",
    "roc_auc_if = roc_auc_score(y_test, scores_if_test_norm)\n",
    "f1_if = f1_score(y_test, pred_if_test)\n",
    "precision_if = precision_score(y_test, pred_if_test, zero_division=0)\n",
    "recall_if = recall_score(y_test, pred_if_test)\n",
    "\n",
    "print(f\"\\nResultados Isolation Forest:\")\n",
    "print(f\"  ROC-AUC: {roc_auc_if:.4f}\")\n",
    "print(f\"  F1-Score: {f1_if:.4f}\")\n",
    "print(f\"  Precision: {precision_if:.4f}\")\n",
    "print(f\"  Recall: {recall_if:.4f}\")\n",
    "print(f\"  Umbral óptimo: {mejor_umbral_if:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PASO 7: MODELO 2 - AUTOENCODER DENSO OPTIMIZADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MODELO 2: AUTOENCODER DENSO\n",
      "================================================================================\n",
      "\n",
      "Resultados Autoencoder:\n",
      "  ROC-AUC: 0.4902\n",
      "  F1-Score: 0.0852\n",
      "  Precision: 0.0654\n",
      "  Recall: 0.1222\n",
      "  Épocas: 50\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MODELO 2: AUTOENCODER DENSO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Entrenar solo con datos normales\n",
    "X_normales_train = X_train[y_train == 0]\n",
    "X_ae_train, X_ae_val = train_test_split(X_normales_train, test_size=0.2, random_state=42)\n",
    "\n",
    "dim_entrada = X_train.shape[1]\n",
    "\n",
    "# Arquitectura optimizada\n",
    "entrada = Input(shape=(dim_entrada,))\n",
    "x = Dense(128, activation='relu')(entrada)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "cuello_botella = Dense(32, activation='relu', name='bottleneck')(x)\n",
    "x = Dense(64, activation='relu')(cuello_botella)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "salida = Dense(dim_entrada, activation='linear')(x)\n",
    "\n",
    "autoencoder = Model(inputs=entrada, outputs=salida)\n",
    "autoencoder.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=0)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=0)\n",
    "\n",
    "historial_ae = autoencoder.fit(\n",
    "    X_ae_train, X_ae_train,\n",
    "    epochs=50,\n",
    "    batch_size=256,\n",
    "    validation_data=(X_ae_val, X_ae_val),\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Calcular error de reconstrucción\n",
    "X_train_reconstruido = autoencoder.predict(X_train, verbose=0)\n",
    "X_test_reconstruido = autoencoder.predict(X_test, verbose=0)\n",
    "\n",
    "error_ae_train = np.mean(np.abs(X_train - X_train_reconstruido), axis=1)\n",
    "error_ae_test = np.mean(np.abs(X_test - X_test_reconstruido), axis=1)\n",
    "\n",
    "# Normalizar\n",
    "scores_ae_train = (error_ae_train - error_ae_train.min()) / (error_ae_train.max() - error_ae_train.min())\n",
    "scores_ae_test = (error_ae_test - error_ae_train.min()) / (error_ae_train.max() - error_ae_train.min())\n",
    "scores_ae_test = np.clip(scores_ae_test, 0, 1)\n",
    "\n",
    "# Optimizar umbral\n",
    "umbrales_ae = np.percentile(scores_ae_train, np.arange(85, 100, 1))\n",
    "mejor_f1_ae = 0\n",
    "mejor_umbral_ae = 0\n",
    "\n",
    "for umbral in umbrales_ae:\n",
    "    pred = (scores_ae_train > umbral).astype(int)\n",
    "    f1 = f1_score(y_train, pred, zero_division=0)\n",
    "    if f1 > mejor_f1_ae:\n",
    "        mejor_f1_ae = f1\n",
    "        mejor_umbral_ae = umbral\n",
    "\n",
    "# Evaluar\n",
    "pred_ae_test = (scores_ae_test > mejor_umbral_ae).astype(int)\n",
    "roc_auc_ae = roc_auc_score(y_test, scores_ae_test)\n",
    "f1_ae = f1_score(y_test, pred_ae_test)\n",
    "precision_ae = precision_score(y_test, pred_ae_test, zero_division=0)\n",
    "recall_ae = recall_score(y_test, pred_ae_test)\n",
    "\n",
    "print(f\"\\nResultados Autoencoder:\")\n",
    "print(f\"  ROC-AUC: {roc_auc_ae:.4f}\")\n",
    "print(f\"  F1-Score: {f1_ae:.4f}\")\n",
    "print(f\"  Precision: {precision_ae:.4f}\")\n",
    "print(f\"  Recall: {recall_ae:.4f}\")\n",
    "print(f\"  Épocas: {len(historial_ae.history['loss'])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.9.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
